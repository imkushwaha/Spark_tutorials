{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Exploration_Analysis_with_ApacheSpark.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1E8HL6G88GfKxysb_cgYbd3yQrP4aNLHg","authorship_tag":"ABX9TyMfWIFGhoCaXabFnHyT/1At"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Module: Getting Started with Spark's RDD\n","\n","- Understanding the role of spark in data analysis\n","\n","- Understanding RDD and their characteristics\n","\n","- Installing Spark Standalone in a local environment\n","\n","- Loading data from a file\n","\n","- Reading data from an RDD\n","\n","In Data Science, 80% of the time spent prepare data, 20% of time spent complain about need for prepare data.\n","\n","### Data Processing Tasks\n","\n","- Parsing fields from text\n","- Accounting for missing values\n","- Identifying and investigating anomalies\n","- Summarizing using tables and charts\n","\n","### Data Complexity\n","\n","#### Small Data (somewhat Messy)\n","\n","- Low data collection frequency\n","- 10s-1000s of rows per day\n","- sometimes involves manual data collection\n","- This kind of data can be handled by Spreadsheets\n","\n","#### Medium Data (High Data Integrity)\n","\n","- High frequency of collection\n","- 100k rows per day\n","- Programmatically collected\n","- ACID properties\n","- Handled by Databases using tool like sql\n","\n","#### Big Data (very messy)\n","\n","- very high frequency of data collection\n","- Millions/Billions of rows per day\n","- Files stored across a cluster of machines\n","- Many many files (webpages, log files)\n","- Handled by Distributed Computing engine like Hadop, MapReduce, Spark\n"],"metadata":{"id":"WXm86vhwLQcM"}},{"cell_type":"markdown","source":["## Here we are going to focus on Handling Big Data by using Apache Spark using Python.\n","\n","### Spark\n","\n","- An engine for data procrssing and analysis. It is General Purpose, Interactive and uses Distributed Computing engine.\n","\n","#### Why General Purpose?\n","\n","- Exploring\n","- Cleaning and Preparing\n","- Applying Machine Learning\n","- Building Data Applications\n","\n","#### Why Interactive?\n","\n","- REPL: Read-Evaluate-Print-Loop\n","- Interactive environments Fast feedback\n","\n","#### Why Distributed Computing?\n","\n","- Process data across a cluster of machines\n","- Integrate with Hadoop\n","\n","\n"],"metadata":{"id":"CxBfciWUOn1o"}},{"cell_type":"markdown","source":["## Spark APIs: Scala, Python and Java\n","\n","Here we will mostly focus on Spark Python API.\n","\n","Almost all data is processed using Resilient Distributed Datasets.\n","\n","- RDDs are the main programming abstraction in spark\n","- RDDS are in-memory collections of objects\n","- With RDDs, you can interact and play with billions of row of data\n","\n","### Spark is made up of a few different components:\n","\n","- Spark Core: The basic functionality of spark RDDs. Spark Core is just a computing engine. It needs two additional components that are A Storage System that stores the data to be processed and A cluster Manager to help Spark run tasks across a cluster of machines. Both of these are plug and play components.\n","\n","For Storage Sytem we can choose Local file System in standalone mode or we can connect it with HDFS if integrating it with Hadoop.\n","\n","For Cluster Manager we can choose Built-in cluster Manager comes in a standalone mode or we could plug in YARN if integrating with Hadoop."],"metadata":{"id":"FwYUIJI7QM8F"}},{"cell_type":"markdown","source":["## Installing Spark\n","\n","### Prerequisites\n","\n","- Java 7 or above\n","- Scala\n","- IPython (Anaconda) then\n","\n","- Go and download Spark binaries\n","- Update environment variables\n","\n","#### Spark Environment Variables\n","\n","- SPARK_HOME: Point to the folder where Spark has been extracted\n","\n","- PATH: %SPARK_HOME%/bin\n","\n","#### Configure iPython Notebook for Spark\n","\n","For this we have to set two more environment variable\n","\n","- PYSPARK_DRIVER_PYTHON: ipython\n","\n","- PYSPARK_DRIVER_PYTHON_OPTS: \"notebook\"\n","\n","\n","\n","\n"],"metadata":{"id":"W-h01-d0S-wW"}},{"cell_type":"markdown","source":["## RDD: Resilient Distributed Datasets\n","\n","- Collection of records which are kept in memory\n","- Primary way by which we integrate data in spark\n","- RDDs have some special characteristics: \n"," -  Partitions, \n"," -  Read-only, \n"," -  Lineage.\n","\n","#### Partitions\n","- RDDs represent data in-memory\n","- Data is divided into partitions\n","- Distributed to multiple machines, called nodes\n","- Nodes are individual machine in cluster\n","- Nodes process data in parallel\n","\n","#### Read-Only\n","- RDDs are immutabel\n","- Only two types of operations\n"," - Transformation: Transform into another RDD\n"," - Action: Request a result\n","\n"," #### Transformation\n"," - A data set loaded inot an RDD\n"," - The user may define a chain of transformations on the dataset\n"," - Load data, Pick only the 3rd column, Sort the values\n"," - Wait until a result is requested before executing any of these transformations till then any RDD only contain metadata.\n","\n"," #### Action\n"," - Request a result using an action\n"," - Such as The first 10 rows, A count, A sum\n"," - Data is processed only when the user requests a result\n"," - The chain of tranformations define earlierr is executed\n"," - Spark keeps a record of the series of transformations requested by the user\n"," - When the action is called upon, it groups the transformations in an efficient way when an Action is requested\n"," - It is Lazy Evaluation of Spark\n","\n","#### Lineage\n","- When created, an RDD just holds metadata: A tranformation and its parent RDD\n","- Every RDD knows where it came from\n","- Lineage can be tarced back all the way to the source\n","- When an action is requested on an RDD\n","- All its parent RDDs are materialized\n","- The characteristic of lineage is what allows RDDs to have resilience and Lazy Evaluation\n","- Resilience: In-built fault tolerance, if something goes wrong, reconstruct from source\n","- Lazy Evaluation: Materialize only when necessary\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"4vX1-FI8WCYL"}},{"cell_type":"markdown","source":["### Module: Transforming and Cleaning Unstructured Data\n","\n","- Transforming data using functional constructs i.e. filter, map and reduce\n","- Cleaning unstructured data\n","- Identifying and removing anomalies, missing values"],"metadata":{"id":"wrVF0swDclB2"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rMI4lImFKw0B","executionInfo":{"status":"ok","timestamp":1651941192671,"user_tz":-330,"elapsed":55077,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}},"outputId":"bfbcc8bf-b775-40e2-ceae-0416dbe1d0e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.3\n","  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 38.5 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=80b04bfa6f20c42d5bc75b5ee14461db43afb29e164a35b4e0fde5becf1f1b6e\n","  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"NYCrimeAnalysis\").getOrCreate()\n","sc = spark.sparkContext"],"metadata":{"id":"YgyLaDIydyoL","executionInfo":{"status":"ok","timestamp":1651941214609,"user_tz":-330,"elapsed":8176,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# load the data and get a quick sense\n","path = \"/content/drive/MyDrive/SAS2PYS/data/NYPD_7_Major_Felony_Incidents.csv\"\n","data = sc.textFile(path)"],"metadata":{"id":"v5z6ldepeBeU","executionInfo":{"status":"ok","timestamp":1651941231933,"user_tz":-330,"elapsed":1231,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data.take(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxMziKhDedF9","executionInfo":{"status":"ok","timestamp":1651941259840,"user_tz":-330,"elapsed":4050,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}},"outputId":"d9b9ba39-8824-4761-b8b7-7a632cb6fab8"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['OBJECTID,Identifier,Occurrence Date,Day of Week,Occurrence Month,Occurrence Day,Occurrence Year,Occurrence Hour,CompStat Month,CompStat Day,CompStat Year,Offense,Offense Classification,Sector,Precinct,Borough,Jurisdiction,XCoordinate,YCoordinate,Location 1',\n"," '1,f070032d,09/06/1940 07:30:00 PM,Friday,Sep,6,1940,19,9,7,2010,BURGLARY,FELONY,D,66,BROOKLYN,N.Y. POLICE DEPT,987478,166141,\"(40.6227027620001, -73.9883732929999)\"',\n"," '2,c6245d4d,12/14/1968 12:20:00 AM,Saturday,Dec,14,1968,0,12,14,2008,GRAND LARCENY,FELONY,G,28,MANHATTAN,N.Y. POLICE DEPT,996470,232106,\"(40.8037530600001, -73.955861904)\"',\n"," '3,716dbc6f,10/30/1970 03:30:00 PM,Friday,Oct,30,1970,15,10,31,2008,BURGLARY,FELONY,H,84,BROOKLYN,N.Y. POLICE DEPT,986508,190249,\"(40.688874254, -73.9918594329999)\"',\n"," '4,638cd7b7,07/18/1972 11:00:00 PM,Tuesday,Jul,18,1972,23,7,19,2012,GRAND LARCENY OF MOTOR VEHICLE,FELONY,F,73,BROOKLYN,N.Y. POLICE DEPT,1005876,182440,\"(40.6674141890001, -73.9220463899999)\"',\n"," '5,6e410287,05/21/1987 12:01:00 AM,Thursday,May,21,1987,0,5,28,2009,GRAND LARCENY,FELONY,K,75,BROOKLYN,N.Y. POLICE DEPT,1017958,182266,\"(40.6668988440001, -73.878495425)\"',\n"," '6,7eebfe3c,02/01/1990 09:00:00 AM,Thursday,Feb,1,1990,9,9,17,2014,GRAND LARCENY,FELONY,K,105,QUEENS,N.Y. POLICE DEPT,1058407,204788,\"(40.7284698170001, -73.7324430589999)\"',\n"," '7,da21f94f,11/13/1990 12:01:00 AM,Tuesday,Nov,13,1990,0,6,7,2007,GRAND LARCENY,FELONY,,73,BROOKLYN,N.Y. HOUSING POLICE,1010272,183760,\"(40.671025464, -73.906195082)\"',\n"," '8,87c99e8c,02/02/1992 04:00:00 PM,Sunday,Feb,2,1992,16,3,27,2012,GRAND LARCENY,FELONY,,101,QUEENS,N.Y. POLICE DEPT,1053678,159044,\"(40.6029515910001, -73.749976261)\"',\n"," '9,495f57e1,08/08/1994 06:00:00 PM,Monday,Aug,8,1994,18,7,31,2008,RAPE,FELONY,A,103,QUEENS,N.Y. POLICE DEPT,1041749,196938,\"(40.707047475, -73.792611904)\"']"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["### Transforming Data with Spark\n","\n","- To transform data in Spark we use a special paradigm called the functional paradigm called the functional paradigm.\n","- As we know, RDD such is a collection of records\n","- Lists and maps in python, all data frames in R might be collections that we have come across before. Any transformation or computation on this collection of objects involve doing something with each item in the collection.\n","- One way of doing this is Imperative way using loops\n","- On the other haand we could use Functional way.\n","- The functional way will perform an operation independently on every element of the record at the same time and return a new set of records. So it doesn't modify each record in place. It will return a new set of records.\n","- Apply the same function to each record\n","- Functional programming allows us to process data in parallel, which is why it fits in really well with a distributed computing system like spark.\n","- Spark uses the functional programming way to actually perform operations on RDDs.\n","- The function can be Explicitly Defined Functions or Lambda Function.\n","\n","\n"],"metadata":{"id":"trgQ_cV2f15F"}},{"cell_type":"markdown","source":["### Functional Programming\n","\n","It basically involves taking a function and applying it on each record of a collection. In spark those collection are RDDs.\n","- Filter\n"," - The function can be used to filter records which match a certain condition.\n"," - The result of the filter operation would be a new RDD in which we have dropped all the records that don't match the condition that we have specified in our boolean function.\n","- Map \n"," - They can be used to map or transform each record to a new record.\n","- Reduce \n"," - They can be used to combine the records in a specified way. For instance recompute a sum.\n"," - The reduce operation is slightly different from the filter and map operations, Which are truly applied in parallel on all records in the RDD. The reduce operation on the other hand is applied on two records at a time.\n"," - Unlike the filter and map operations which take in functions with a single argument, the argument representing one record but the reduce operation takes in a function with two arguments.\n"],"metadata":{"id":"O7mynofldqQC"}},{"cell_type":"markdown","source":["### Filter"],"metadata":{"id":"ELrss9Nrg8MP"}},{"cell_type":"code","source":["# Filtering the header row\n","header = data.first()\n","print(header)"],"metadata":{"id":"TyVUKuGIfAPa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651941281017,"user_tz":-330,"elapsed":1265,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}},"outputId":"52815397-c18c-4da6-f350-34a7cb27608b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["OBJECTID,Identifier,Occurrence Date,Day of Week,Occurrence Month,Occurrence Day,Occurrence Year,Occurrence Hour,CompStat Month,CompStat Day,CompStat Year,Offense,Offense Classification,Sector,Precinct,Borough,Jurisdiction,XCoordinate,YCoordinate,Location 1\n"]}]},{"cell_type":"code","source":["dataWoHeader = data.filter(lambda x: x != header)"],"metadata":{"id":"TKdrZy8khO6V","executionInfo":{"status":"ok","timestamp":1651941286550,"user_tz":-330,"elapsed":749,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["dataWoHeader.first()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"RY8BL47ghp0i","executionInfo":{"status":"ok","timestamp":1651941291025,"user_tz":-330,"elapsed":521,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}},"outputId":"b2450efd-f3cf-49bd-d658-89ccc6e26034"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1,f070032d,09/06/1940 07:30:00 PM,Friday,Sep,6,1940,19,9,7,2010,BURGLARY,FELONY,D,66,BROOKLYN,N.Y. POLICE DEPT,987478,166141,\"(40.6227027620001, -73.9883732929999)\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Transforming records from string to named tuples"],"metadata":{"id":"TtifZH1xovPN"}},{"cell_type":"code","source":["# Parse the rows to extract fields\n","dataWoHeader.map(lambda x:x.split(\",\")).take(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2bhB8Gnh5b3","executionInfo":{"status":"ok","timestamp":1651941319592,"user_tz":-330,"elapsed":514,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}},"outputId":"78e51669-a147-4894-f776-28cc867e2767"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['1',\n","  'f070032d',\n","  '09/06/1940 07:30:00 PM',\n","  'Friday',\n","  'Sep',\n","  '6',\n","  '1940',\n","  '19',\n","  '9',\n","  '7',\n","  '2010',\n","  'BURGLARY',\n","  'FELONY',\n","  'D',\n","  '66',\n","  'BROOKLYN',\n","  'N.Y. POLICE DEPT',\n","  '987478',\n","  '166141',\n","  '\"(40.6227027620001',\n","  ' -73.9883732929999)\"'],\n"," ['2',\n","  'c6245d4d',\n","  '12/14/1968 12:20:00 AM',\n","  'Saturday',\n","  'Dec',\n","  '14',\n","  '1968',\n","  '0',\n","  '12',\n","  '14',\n","  '2008',\n","  'GRAND LARCENY',\n","  'FELONY',\n","  'G',\n","  '28',\n","  'MANHATTAN',\n","  'N.Y. POLICE DEPT',\n","  '996470',\n","  '232106',\n","  '\"(40.8037530600001',\n","  ' -73.955861904)\"'],\n"," ['3',\n","  '716dbc6f',\n","  '10/30/1970 03:30:00 PM',\n","  'Friday',\n","  'Oct',\n","  '30',\n","  '1970',\n","  '15',\n","  '10',\n","  '31',\n","  '2008',\n","  'BURGLARY',\n","  'FELONY',\n","  'H',\n","  '84',\n","  'BROOKLYN',\n","  'N.Y. POLICE DEPT',\n","  '986508',\n","  '190249',\n","  '\"(40.688874254',\n","  ' -73.9918594329999)\"'],\n"," ['4',\n","  '638cd7b7',\n","  '07/18/1972 11:00:00 PM',\n","  'Tuesday',\n","  'Jul',\n","  '18',\n","  '1972',\n","  '23',\n","  '7',\n","  '19',\n","  '2012',\n","  'GRAND LARCENY OF MOTOR VEHICLE',\n","  'FELONY',\n","  'F',\n","  '73',\n","  'BROOKLYN',\n","  'N.Y. POLICE DEPT',\n","  '1005876',\n","  '182440',\n","  '\"(40.6674141890001',\n","  ' -73.9220463899999)\"'],\n"," ['5',\n","  '6e410287',\n","  '05/21/1987 12:01:00 AM',\n","  'Thursday',\n","  'May',\n","  '21',\n","  '1987',\n","  '0',\n","  '5',\n","  '28',\n","  '2009',\n","  'GRAND LARCENY',\n","  'FELONY',\n","  'K',\n","  '75',\n","  'BROOKLYN',\n","  'N.Y. POLICE DEPT',\n","  '1017958',\n","  '182266',\n","  '\"(40.6668988440001',\n","  ' -73.878495425)\"'],\n"," ['6',\n","  '7eebfe3c',\n","  '02/01/1990 09:00:00 AM',\n","  'Thursday',\n","  'Feb',\n","  '1',\n","  '1990',\n","  '9',\n","  '9',\n","  '17',\n","  '2014',\n","  'GRAND LARCENY',\n","  'FELONY',\n","  'K',\n","  '105',\n","  'QUEENS',\n","  'N.Y. POLICE DEPT',\n","  '1058407',\n","  '204788',\n","  '\"(40.7284698170001',\n","  ' -73.7324430589999)\"'],\n"," ['7',\n","  'da21f94f',\n","  '11/13/1990 12:01:00 AM',\n","  'Tuesday',\n","  'Nov',\n","  '13',\n","  '1990',\n","  '0',\n","  '6',\n","  '7',\n","  '2007',\n","  'GRAND LARCENY',\n","  'FELONY',\n","  '',\n","  '73',\n","  'BROOKLYN',\n","  'N.Y. HOUSING POLICE',\n","  '1010272',\n","  '183760',\n","  '\"(40.671025464',\n","  ' -73.906195082)\"'],\n"," ['8',\n","  '87c99e8c',\n","  '02/02/1992 04:00:00 PM',\n","  'Sunday',\n","  'Feb',\n","  '2',\n","  '1992',\n","  '16',\n","  '3',\n","  '27',\n","  '2012',\n","  'GRAND LARCENY',\n","  'FELONY',\n","  '',\n","  '101',\n","  'QUEENS',\n","  'N.Y. POLICE DEPT',\n","  '1053678',\n","  '159044',\n","  '\"(40.6029515910001',\n","  ' -73.749976261)\"'],\n"," ['9',\n","  '495f57e1',\n","  '08/08/1994 06:00:00 PM',\n","  'Monday',\n","  'Aug',\n","  '8',\n","  '1994',\n","  '18',\n","  '7',\n","  '31',\n","  '2008',\n","  'RAPE',\n","  'FELONY',\n","  'A',\n","  '103',\n","  'QUEENS',\n","  'N.Y. POLICE DEPT',\n","  '1041749',\n","  '196938',\n","  '\"(40.707047475',\n","  ' -73.792611904)\"'],\n"," ['10',\n","  '31c0b727',\n","  '10/26/1994 12:01:00 AM',\n","  'Wednesday',\n","  'Oct',\n","  '26',\n","  '1994',\n","  '0',\n","  '6',\n","  '4',\n","  '2008',\n","  'GRAND LARCENY',\n","  'FELONY',\n","  'D',\n","  '17',\n","  'MANHATTAN',\n","  'N.Y. POLICE DEPT',\n","  '992029',\n","  '213332',\n","  '\"(40.7522284',\n","  ' -73.971924858)\"']]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import csv\n","from io import StringIO\n","from collections import namedtuple"],"metadata":{"id":"1RDzZtozpE_8","executionInfo":{"status":"ok","timestamp":1651941385636,"user_tz":-330,"elapsed":513,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["print(header)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"674fPbcMqeoM","executionInfo":{"status":"ok","timestamp":1651941394750,"user_tz":-330,"elapsed":515,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}},"outputId":"aa3c3e8b-929b-490c-81c4-bcb35ddea823"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["OBJECTID,Identifier,Occurrence Date,Day of Week,Occurrence Month,Occurrence Day,Occurrence Year,Occurrence Hour,CompStat Month,CompStat Day,CompStat Year,Offense,Offense Classification,Sector,Precinct,Borough,Jurisdiction,XCoordinate,YCoordinate,Location 1\n"]}]},{"cell_type":"code","source":["fields = header.replace(\" \", \"_\").replace(\"/\",\"_\").split(\",\")"],"metadata":{"id":"DT5Q18VUplpn","executionInfo":{"status":"ok","timestamp":1651941402428,"user_tz":-330,"elapsed":514,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(fields)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zs_tmVRyqpKK","executionInfo":{"status":"ok","timestamp":1651941405097,"user_tz":-330,"elapsed":7,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}},"outputId":"770711bf-8288-4d27-eb23-686bbf69c666"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['OBJECTID', 'Identifier', 'Occurrence_Date', 'Day_of_Week', 'Occurrence_Month', 'Occurrence_Day', 'Occurrence_Year', 'Occurrence_Hour', 'CompStat_Month', 'CompStat_Day', 'CompStat_Year', 'Offense', 'Offense_Classification', 'Sector', 'Precinct', 'Borough', 'Jurisdiction', 'XCoordinate', 'YCoordinate', 'Location_1']\n"]}]},{"cell_type":"code","source":["crime = namedtuple('crime', fields)"],"metadata":{"id":"8BeLtpTBqroI","executionInfo":{"status":"ok","timestamp":1651941409195,"user_tz":-330,"elapsed":523,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def parse(row):\n","  reader = csv.reader(StringIO(row))\n","  row=reader.next()\n","  return crime(row)"],"metadata":{"id":"caiMmhZfq-4j","executionInfo":{"status":"ok","timestamp":1651941417950,"user_tz":-330,"elapsed":573,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["crimes=dataWoHeader.map(parse)"],"metadata":{"id":"fXl0qLbZrHUs","executionInfo":{"status":"ok","timestamp":1651941422295,"user_tz":-330,"elapsed":516,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["crimes.first()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"35pd38lGr3Xd","executionInfo":{"status":"error","timestamp":1651913714688,"user_tz":-330,"elapsed":1438,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}},"outputId":"4067fd37-eb17-4e17-c800-09e446faf765"},"execution_count":40,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-0863997bf5e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11) (e9b69ac26928 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-32-0ff3d0c9e752>\", line 3, in parse\nAttributeError: '_csv.reader' object has no attribute 'next'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-32-0ff3d0c9e752>\", line 3, in parse\nAttributeError: '_csv.reader' object has no attribute 'next'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"]}]},{"cell_type":"markdown","source":["## Identifying missing values\n","#### Filtering records with missing values"],"metadata":{"id":"DcbJ2xzAbGWV"}},{"cell_type":"code","source":["crimes.map(lambda x:x.Offense).countByValue()"],"metadata":{"id":"uKoqRV0cr5zH","executionInfo":{"status":"ok","timestamp":1651943391022,"user_tz":-330,"elapsed":616,"user":{"displayName":"Upendra Kumar","userId":"02077147260212980230"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["crimes.map(lambda x:x.Occurrence_Year).countByValue()"],"metadata":{"id":"pi9Met0texZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["crimesFiltered = crimes.filter(lambda x: not (x.Offense==\"NA\" or x.Occurrence_Year==' '))\\\n","                       .filter(lambda x: int(x.Occurrence_Year)>=2006)"],"metadata":{"id":"2GApC7NJgVsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["crimesFiltered.map(lambda x:x.Occurrence_Year).countByValue()"],"metadata":{"id":"d1WYhi2nhXCo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Identifying anomalies\n","#### Filtering records with anomalies"],"metadata":{"id":"Opf9o7XVhyuS"}},{"cell_type":"code","source":[""],"metadata":{"id":"X-7LgCsZhjQF"},"execution_count":null,"outputs":[]}]}