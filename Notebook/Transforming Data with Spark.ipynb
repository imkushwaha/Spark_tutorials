{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a10cad",
   "metadata": {},
   "source": [
    "## Transforming Data with Spark\n",
    "\n",
    "- To transform data in spark, we use a special paradigm called the functional paradigm.\n",
    "\n",
    "\n",
    "- As we know, RDD is a collections of records. Any transformation or computation on this collection of objects involves doing something with each item in the collection.\n",
    "\n",
    "\n",
    "- One way of doing something with each item in the collection is the imperative way, which is using for loops or while loops.\n",
    "\n",
    "\n",
    "- In this method, we would basically take each element at a time perform some transformation on it then move on to the next element, transform that element and so on until you reach the end of the collection.\n",
    "\n",
    "\n",
    "- In the imperative way, we perform an operation sequentially on each element of the collection. This allows you to keep track of which element we are currently operating on and how mamy we have already finished and how many are left to go.\n",
    "\n",
    "\n",
    "- But this method doesn't involve any parallelism and hence might not be taking advantage of the performance advantages that our distributed computing system might provide.\n",
    "\n",
    "\n",
    "- On the other hand, we could use a functional way.\n",
    "\n",
    "\n",
    "- The functional way will perform an operation independently on every element of the records at the same time and return a new set of records. So it doesn't modify each record in place.\n",
    "\n",
    "\n",
    "- In this method, we are basically taking a function that defines some logic and applying that function on each record in the collection at the same time.\n",
    "\n",
    "\n",
    "- This functional programming allows us to process data in parallel.\n",
    "\n",
    "\n",
    "- Spark uses the functional programming way to actually perform operations on RDDs.\n",
    "\n",
    "\n",
    "- The function that we might apply on each record could be an explicitly defined function.\n",
    "\n",
    "\n",
    "- Such a function would basically act on each record so it should have a single argument and once this function is applied we will get a new RDD whose records will depend upon the results which are returned bu this function.\n",
    "\n",
    "\n",
    "- Rather than defining an explicit function, we can also use lambda functions.\n",
    "\n",
    "\n",
    "- Lambda function are normally defined by an input on one side and an expression which performs some computation on the input and returns an output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751d682",
   "metadata": {},
   "source": [
    "### Functional Programming\n",
    "\n",
    "- Filter: The function can be used to filter records which match a certain condition.\n",
    "\n",
    "\n",
    "- Map: They can be used to map or transform each record to a new record.\n",
    "\n",
    "\n",
    "- Reduce: They can be used to combine the records in a specified way, for instance, recompute a sum.\n",
    "\n",
    "#### Filter:\n",
    "\n",
    "- Filter records matching a given condition.\n",
    "\n",
    "\n",
    "- The filter operation takes in a function which returns a Boolean value.\n",
    "\n",
    "\n",
    "- IT will retunrn either true or false for each record that it processes.\n",
    "\n",
    "\n",
    "- If the function returns true, then you would keep the record otherwise, you would drop the record.\n",
    "\n",
    "\n",
    "- The result of the filter operation would be a new RDD in which you have dropped all the records that didn't match the condition that you have specified in your Boolean function.\n",
    "\n",
    "\n",
    "- This operation is useful to filter out a header row in a dat set or you might use it to select rows corresponding to a specific value.\n",
    "\n",
    "\n",
    "#### Map:\n",
    "\n",
    "- Takes a record and transform a record to another record.\n",
    "\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "- The reduce operation is used to combine records in an RDD in a specified way for instance if you wanted to combine the sum of some values or a maximum or a minimum.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The reduce operation is slightly different from the filter and map operations, which are truly applied in parallel on all records in the RDD.\n",
    "\n",
    "\n",
    "- The reduce operation on the other hand is applied on two records at a time. Therefore, unlike the filter and map operations, which take in functions with a single argument the argument representing one record, the reduce opeartion takes in a function with two arguments.\n",
    "\n",
    "#### Combining Records\n",
    "\n",
    "\n",
    "- You would start by applying the function on the first two records in the RDD and get the result. Let's say the function was sum, You would get the sum of the first two arguments then you would apply the same function under the result of the first application and the second record.\n",
    "\n",
    "\n",
    "- In each step, you would apply the function on the result from the previous step and the current record. You would do this until you have combined all the records\n",
    "\n",
    "\n",
    "- Now RDD as you know are partitioned, so the data are split across multiple nodes. In such a case, the reduce operation is applied on each partition then the results from all the partitions are taken to one single node, and reduce operation is applied on those results again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36333449",
   "metadata": {},
   "source": [
    "##### Filter and Map are Transformation and Reduce is  Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4332b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10(ipykernel)",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
